## Need to:
- explain what they're doing so I can replicate
	- especially the numerical parts
	- summarize in report
	- we are mimic-ing their work this summer, so understand it
- expand out NS-$\alpha$ fully (ie expand $\nabla, \Delta,$ etc)

## ai summary:
### Simplified Explanation of the Paper

#### **Main Idea**
The paper presents a new way to combine **data assimilation** (a technique used to improve predictions by blending observations with models) with **deep neural networks (DNNs)**. The goal is to make predictions faster and cheaper while maintaining accuracy. 

- **Traditional Data Assimilation (Nudging):**  
  - Used in weather forecasting, oceanography, etc., to correct model predictions using real-world observations.  
  - Requires solving differential equations at every step, which can be computationally expensive.  

- **Proposed Method:**  
  - Instead of solving the equations repeatedly, the authors train a DNN to mimic the "nudging" process.  
  - The DNN learns how to adjust predictions based on observations, making the process much faster after training.  
  - The DNN is trained using data generated by the traditional nudging method, so it learns to behave similarly but without the heavy computations.  

This is **not a modification of traditional neural networks** but rather a new way to use them to replace an existing mathematical technique (nudging) for efficiency.

---

#### **Key Variables and Concepts**
Here’s a breakdown of the main variables and terms:

1. **Dynamical System (e.g., Lorenz 63/Lorenz 96 models):**  
   - Represents real-world phenomena like weather or fluid flow.  
   - Equations like:  
     \[
     \frac{dx}{dt} = \sigma(y - x), \quad \frac{dy}{dt} = x(\rho - z) - y, \quad \frac{dz}{dt} = xy - \beta z
     \]
     (These are the Lorenz 63 equations, a classic chaotic system.)  

2. **Truth/Reference Solution (\(u\) or \(x, y, z\)):**  
   - The "real" state of the system (unknown in practice, but simulated here for testing).  

3. **Observations (\(I_M u\)):**  
   - Partial/noisy measurements of the true state (e.g., only temperature or wind speed at certain locations).  
   - \(I_M\) is an operator that selects which parts of the state are observed.  

4. **Nudging Solution (\(w\) or \(\bar{x}, \bar{y}, \bar{z}\)):**  
   - A corrected version of the model that "nudges" toward observations using a parameter \(\mu\).  
   - Solves modified equations like:  
     \[
     \frac{d\bar{x}}{dt} = \sigma(\bar{y} - \bar{x}) - \mu(\bar{x} - x_{\text{observed}}).
     \]  

5. **DNN (\(w_{\text{DNN}}\)):**  
   - A neural network trained to predict the next state (\(w(t_{k+1})\)) given the current state (\(w(t_k)\)) and observations.  
   - Input: \([w(t_k), I_M u(t_k)]\) → Output: \(w(t_{k+1})\).  

6. **Error Metrics (RMSE):**  
   - Measures how close the DNN or nudging predictions are to the true solution.  

---

#### **Why This Matters**
- **Speed:** Once trained, the DNN runs much faster than solving differential equations repeatedly.  
- **Accuracy:** The DNN performs almost as well as traditional nudging (see Table 1 in the paper).  
- **Flexibility:** The method can work with sparse/noisy data (e.g., only 10% of the system observed).  

#### **Analogy**
Imagine you’re driving a car with a faulty GPS.  
- **Nudging:** Constantly recalculating your route using complex math.  
- **DNN Approach:** Letting a trained AI (like Google Maps) predict corrections instantly, without the math.  

The paper shows this AI can be trained to work almost as well as the math-heavy original.  

Let me know if you'd like any part clarified further!

![[Pasted image 20250608212814.png]]
